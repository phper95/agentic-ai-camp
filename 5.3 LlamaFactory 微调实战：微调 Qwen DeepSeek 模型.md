
# 5.3 LlamaFactory 微调实战：微调 Qwen/DeepSeek 模型

> **导语**：理论的尽头是实践。我们已经理解了 LoRA 的原理，并学会了如何“指挥”GPT-4 为我们生产高质量的微调数据。现在，万事俱备，只欠“开炉炼丹”。本章将是一次纯粹的、从头到尾的动手实战。我们将使用 **LlamaFactory**，一个在开源社区广受欢迎的、对新手极其友好的“一站式”大模型微调框架，将我们上一章制作的“AI 皮肤科医生”数据集，真正地“炼”到一个强大的开源大模型（如 Qwen1.5, DeepSeek-V2）中。你将学会如何配置环境、设置参数、启动训练、并最终测试你亲手微调出来的、独一无二的垂直领域专家模型。系好安全带，我们的“炼丹”之旅，正式开始！

## 目录
1.  **“炼丹炉”的选择：为什么是 LlamaFactory？**
    *   一体化、易用性、可扩展性
    *   支持丰富的模型、数据集和微调方法
2.  **第一步：环境搭建与准备**
    *   硬件要求：一块拥有至少 24GB 显存的 NVIDIA 显卡（如 RTX 3090/4090）
    *   克隆 LlamaFactory 仓库
    *   安装依赖：`pip install -e .[torch,deepspeed]`
3.  **第二步：数据与模型的准备**
    *   **数据集**：将我们上一章生成的 `medical_data_sharegpt.jsonl` 文件，以及一个描述该数据集信息的 `dataset_info.json` 文件，放入 LlamaFactory 的 `data` 目录。
    *   **基础模型**：从 Hugging Face Hub 下载你想要微调的开源模型（如 `Qwen/Qwen1.5-7B-Chat`），或使用 LlamaFactory 自动下载。
4.  **第三步：配置你的“炼丹”参数**
    *   LlamaFactory 的两种启动方式：Web UI vs. 命令行
    *   **Web UI 详解**：
        *   `Model` (模型)、`Dataset` (数据集) 的选择
        *   `Finetuning method` (微调方法): 选择 `lora`
        *   `Hyperparameters` (超参数): `learning_rate`, `num_train_epochs`, `per_device_train_batch_size`
        *   `LoRA settings`: `lora_rank (r)`, `lora_alpha`, `lora_target`
    *   **导出命令行**：从 Web UI 生成可执行的命令行指令。
5.  **第四步：“开炉炼丹”——启动微调！**
    *   执行 `llamafactory-cli train` 命令
    *   观察终端输出：监控 `loss` (损失) 的下降趋势
    *   训练完成：在哪里找到我们炼出的“仙丹”（LoRA 适配器）？
6.  **第五步：验证“丹药”效果——推理与对比**
    *   **合并模型（可选）**：将 LoRA 适配器与基础模型合并，得到一个完整的、微调后的新模型。
    *   **直接加载适配器进行推理**：使用 LlamaFactory 的 Web UI 或命令行，加载**基础模型**和**LoRA 适配器**，进行对话测试。
    *   **效果对比**：
        *   **微调前**：向原始的基础模型提问“我脸上长痘痘怎么办？”
        *   **微调后**：向加载了我们 LoRA 适配器的模型提问同样的问题。
        *   直观感受模型在**专业性、严谨性和同理心**上的巨大变化。
7.  **总结：你已掌握了模型定制的核心技能**

---

## 1. “炼丹炉”的选择：为什么是 LlamaFactory？

要进行微调，我们需要一个能处理数据加载、模型构建、训练循环、梯度更新等一系列复杂工作的框架。在开源社区，有许多优秀的微调框架，如 `huggingface/transformers`, `axolotl`, `SFT-Trainer` 等。

我们选择 **LlamaFactory**，因为它：
*   **一体化**：它将数据处理、模型训练、模型评估、模型推理等所有环节都集成在了一起，提供了一个“一站式”的解决方案。
*   **易用性**：它同时提供了非常直观的 **Web UI** 和强大的**命令行**两种操作方式。对于新手，可以通过点选式的 Web UI 快速上手；对于高级用户，可以通过命令行实现自动化和脚本化。
*   **模型和方法支持广泛**：它支持目前市面上几乎所有主流的开源大模型（Llama, Qwen, DeepSeek, Yi, ChatGLM ...）和所有主流的 PEFT 方法（LoRA, QLoRA, Full-tuning ...）。

## 2. 第一步：环境搭建与准备

**硬件要求**

LoRA 微调虽然高效，但依然需要一块性能不错的 NVIDIA 显卡。为了能顺利地微调 7B（70亿参数）规模的模型，建议使用拥有**至少 24GB 显存**的显卡，如消费级的 **RTX 3090, RTX 4090** 或专业级的 A100 等。

**克隆 LlamaFactory 仓库**

```bash
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
```

**安装依赖**

LlamaFactory 的依赖较多，建议在一个干净的 conda 或 venv 虚拟环境中进行安装。

```bash
# -e 表示以可编辑模式安装，方便我们修改代码
# [torch,deepspeed] 表示同时安装 torch 和 deepspeed 相关的可选依赖
# deepspeed 是一个能进一步优化训练性能的库
pip install -e .[torch,deepspeed]
```
安装过程可能需要一些时间。

## 3. 第二步：数据与模型的准备

**放置数据集**

1.  **复制数据文件**：将我们上一章制作好的 `medical_data_sharegpt.jsonl` 文件，复制到 LlamaFactory 的 `data/` 目录下。
2.  **创建数据集描述文件**：在 `data/` 目录下，有一个 `dataset_info.json` 文件，这是 LlamaFactory 的“数据集注册表”。我们需要在其中为我们的新数据集添加一条记录。

   打开 `data/dataset_info.json`，在 JSON 文件的最上方，添加如下内容：

   ```json
   {
     "medical_sharegpt": {
       "file_name": "medical_data_sharegpt.jsonl",
       "columns": {
         "prompt": "conversations[0].value",
         "query": "conversations[1].value",
         "response": "conversations[2].value",
         "history": []
       },
       "type": "sharegpt"
     },
     // ... 保留文件原有的其他数据集定义 ...
   }
   ```
   *   `"medical_sharegpt"`: 我们为这个数据集起的名字，将在 UI 中看到。
   *   `"file_name"`: 指向我们的数据文件名。
   *   `"type": "sharegpt"`: 告诉 LlamaFactory 这个文件遵循 ShareGPT 格式，它会用相应的解析器来读取。

**选择基础模型**

我们选择一个强大的、开源的、尺寸适中的基础模型。阿里巴巴的 **`Qwen/Qwen1.5-7B-Chat`** 是一个非常不错的选择。你**不需要**手动下载它，LlamaFactory 会在训练开始时自动从 Hugging Face Hub 下载。

## 4. 第三步：配置你的“炼丹”参数

LlamaFactory 提供了非常方便的 Web UI 来进行配置。

**启动 Web UI**

在 `LLaMA-Factory` 的根目录下，运行：

```bash
llamafactory-cli webui
```
然后在浏览器中打开 `http://127.0.0.1:7860`，你就能看到 LlamaFactory 的界面了。

**Web UI 详解**

在 Web UI 顶部的 Tab 中，选择 **Train**。然后按照以下步骤进行配置：

1.  **Model and Dataset**
    *   **Model name**: `Qwen/Qwen1.5-7B-Chat` (可以直接从下拉菜单中搜索并选择)
    *   **Finetuning method**: `lora`
    *   **Dataset**: 在下拉列表中找到并选择我们刚刚定义的 `medical_sharegpt`。

2.  **Hyperparameters (Training)**
    *   **Learning rate**: `2e-4` (这是一个常用的 LoRA 学习率)
    *   **Num train epochs**: `3.0` (我们将整个数据集训练 3 遍)
    *   **Per device train batch size**: `2` (根据你的显存调整，如果显存小，就调低为 1)
    *   **Quantization bit**: `None` (我们暂时不使用 QLoRA 量化，以保证最高精度)
    *   **Gradient accumulation steps**: `8` (梯度累积，可以让你在小显存下模拟更大的 batch size)

3.  **Hyperparameters (LoRA)**
    *   **LoRA rank (r)**: `8` (一个较小的 `r`，训练更快，生成的适配器文件也更小)
    *   **LoRA alpha**: `16` (通常设置为 `r` 的两倍)
    *   **LoRA target modules**: 点击 `q_proj` 和 `v_proj`。这告诉 LoRA 要将“适配器”注入到模型的哪些线性层上。对于大多数 Transformer 模型，注入到 `q_proj` (Query) 和 `v_proj` (Value) 层是最高效的选择。

4.  **Output Settings**
    *   **Output dir**: `saves/Qwen1.5-7B-Chat/lora/medical-expert-v1` (为我们这次训练的模型指定一个清晰的输出路径)

**导出命令行**

在你配置完所有参数后，一个最佳实践是点击页面底部的 **Show command** 按钮。它会生成一个包含了你所有配置的、等价的命令行指令。

```bash
llamafactory-cli train \
    --model_name_or_path Qwen/Qwen1.5-7B-Chat \
    --do_train \
    --dataset medical_sharegpt \
    --finetuning_type lora \
    --output_dir saves/Qwen1.5-7B-Chat/lora/medical-expert-v1 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 100 \
    --learning_rate 2e-4 \
    --num_train_epochs 3.0 \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_target q_proj,v_proj \
    --fp16
```
将这个命令复制下来。使用命令行来启动训练是更推荐的方式，因为它更稳定，且便于记录和复现。

## 5. 第四步：“开炉炼丹”——启动微调！

关闭 Web UI (`Ctrl+C`)，然后在终端中粘贴并执行我们刚刚生成的命令。

训练开始了！你会看到 LlamaFactory 开始自动下载模型、处理数据，然后进入训练循环。终端会不断打印出类似下面的日志：

```
...
[INFO|trainer.py:2406] ***** Running training *****
[INFO|trainer.py:2407]   Num examples = 500
[INFO|trainer.py:2408]   Num Epochs = 3
[INFO|trainer.py:2409]   Instantaneous batch size per device = 2
...
{'loss': 1.583, 'learning_rate': 0.00019, 'epoch': 0.1}
{'loss': 1.235, 'learning_rate': 0.00018, 'epoch': 0.2}
...
```
你需要重点关注的是 `loss` (损失) 这个值。一个健康的训练过程，`loss` 值应该会**持续稳定地下降**。如果 `loss` 不下降或者剧烈波动，说明你的学习率或其他超参数可能设置不当。

根据你的数据集大小和硬件性能，训练过程可能持续几分钟到几小时不等。

**训练完成**

当训练结束后，进入你之前设置的 `output_dir` (`saves/Qwen1.5-7B-Chat/lora/medical-expert-v1`)。你会在这里找到我们炼出的“仙丹”—— LoRA 适配器文件，其中最核心的是 `adapter_model.safetensors` 和 `adapter_config.json`。

## 6. 第五步：验证“丹药”效果——推理与对比

现在，让我们来检验一下我们亲手微调的模型的“疗效”。

**使用 LlamaFactory 的 Web UI 进行推理**

1.  重新启动 Web UI：`llamafactory-cli webui`
2.  这次，选择顶部的 **Chat** Tab。
3.  **Model and Adapter**:
    *   **Model name**: `Qwen/Qwen1.5-7B-Chat` (选择我们用的**基础**模型)
    *   **Adapter path**: 点击下拉菜单，选择我们刚刚训练好的 `medical-expert-v1`。
4.  点击 **Load model**。
5.  现在，你可以和你的“AI 皮肤科医生”对话了！

**效果对比**

*   **微调前**：
    *   在 **Chat** 界面，**不要加载任何 Adapter**，直接加载 `Qwen/Qwen1.5-7B-Chat` 基础模型。
    *   向它提问：“医生你好，我脸上最近老长痘痘，怎么办啊？”
    *   观察它的回答。它可能会给一些通用的、正确的建议，但语气会比较“机械”，并且很可能不会有“必须咨询专业医生”这样的严谨性提示。

*   **微调后**：
    *   加载我们训练好的 `medical-expert-v1` 适配器。
    *   提出**完全相同**的问题。
    *   观察它的回答。你会发现，它的回答风格会惊人地贴近我们数据集中定义的模式：**富有同理心、会追问细节、分析可能性、并且一定会附上那句至关重要的“风险提示”**。

这种行为模式上的鲜明差异，直观地证明了我们微调的成功。我们没有教模型新的皮肤病知识，但我们成功地“**驯化**”了它的行为，让它成为了一个合格的、遵循特定工作流程的“专业角色”。

## 7. 总结：你已掌握了模型定制的核心技能

恭喜你！从准备数据、配置参数、启动训练，到最终验证效果，你已经完整地走通了 LoRA 微调的全流程。

你不再只是一个大模型的使用者，你已经成为一个能够根据特定需求，**亲手定制和优化**模型的“炼丹师”。你掌握了在成本和效果之间取得最佳平衡的、当前最主流的模型优化技术。

这项技能，将是你构建真正专业、强大、独一无二的 Agentic AI 应用的坚实基础。在下一章，我们将探讨如何科学地评估我们微调出的模型的效果，以及如何将其部署到生产环境中。
