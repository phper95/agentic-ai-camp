
# 4.3 Langfuse 集成实战：追踪 OpenAI、LangChain 和 LangGraph 应用

> **导语**：在上一章，我们已经成功地在本地运行了 Langfuse，并初步领略了其强大的追踪能力。现在，是时候将这项能力应用到我们之前构建的各种 AI 应用中了。本章将是一次聚焦于“集成”的实战演练。我们将分别针对三种最主流的应用类型——基于原生 OpenAI API 的应用、基于 LangChain 的应用、以及基于 LangGraph 的复杂应用——提供清晰、可直接复用的集成代码范例。通过本章的学习，你将拥有一个可以随时查阅的“集成手册”，无论你未来的项目采用何种架构，你都能胸有成竹地为其装上 Langfuse 这双“火眼金睛”。

## 目录
1.  **集成模式回顾：手动 vs. 自动**
    *   **手动追踪**：通过 `langfuse.trace()` 等方法，精确控制每一个 `span` 和 `generation` 的创建。最灵活，也最繁琐。
    *   **自动追踪**：通过 `LangfuseCallbackHandler`，利用框架（如 LangChain）的回调机制，自动捕获和记录所有事件。最便捷，也最“魔法”。
2.  **场景一：追踪原生 OpenAI 应用**
    *   **目标**：追踪一个包含“Function Calling”的、使用原生 `openai` 库的对话机器人。
    *   **集成策略**：手动追踪。
    *   **代码实战**：
        *   使用 `langfuse.trace()` 包裹整个对话回合。
        *   使用 `trace.generation()` 手动记录对 `client.chat.completions.create` 的调用，并捕获其 `input`, `output`, `usage` 等信息。
        *   使用 `trace.span()` 包裹工具的实际执行过程。
3.  **场景二：追踪标准 LangChain 应用**
    *   **目标**：追踪一个使用 `AgentExecutor` 构建的 ReAct 风格 Agent。
    *   **集成策略**：自动追踪。
    *   **代码实战**：
        *   初始化 `LangfuseCallbackHandler`。
        *   在调用 `agent_executor.invoke()` 或 `.stream()` 时，通过 `config` 参数传入 `handler`。
        *   一行代码，搞定一切。
4.  **场景三：追踪复杂 LangGraph 应用**
    *   **目标**：追踪我们构建的“DeepResearch”或“旅小智”等多智能体、多节点协作图。
    *   **集成策略**：自动追踪。
    *   **代码实战**：
        *   与 LangChain 应用完全相同的集成方式！
        *   在调用 `app.invoke()` 或 `.stream()` 时，传入 `LangfuseCallbackHandler`。
        *   在 Langfuse UI 中观察 LangGraph 的每个节点（`node`）、边（`edge`）和状态变化如何被自动追踪和可视化。
5.  **追踪元数据（Metadata）：为你的 Trace 添加“标签”和“上下文”**
    *   为什么需要元数据？（用于筛选、分组、分析）
    *   **Trace/Span/Generation 级别**：在创建时传入 `metadata` 字典。
    *   **Callback Handler 级别**：在初始化 `LangfuseCallbackHandler` 时传入 `user_id`, `session_id`, `tags` 等参数。
6.  **总结：一个 Handler，追踪万物**

---
*本章所有代码示例均假设你已经按照 `4.2` 节的说明，在本地 `http://localhost:3000` 启动了 Langfuse 服务，并已在环境变量中配置了 `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, 和 `LANGFUSE_HOST`。*

## 1. 集成模式回顾：手动 vs. 自动

在深入代码之前，我们再次明确 Langfuse 的两种核心集成模式：

*   **手动追踪 (Manual Tracing)**
    *   **何时使用？** 当你的应用没有使用 LangChain，而是直接调用 OpenAI、Anthropic、Gemini 等模型的原生 API 时。
    *   **如何工作？** 你需要像在 `4.2` 节的第一个例子中那样，在代码的特定位置，显式地调用 `langfuse.trace()`, `langfuse.span()`, `langfuse.generation()` 等方法，来告诉 Langfuse “追踪开始”、“追踪结束”、“这是一个 LLM 调用”等。
    *   **优缺点**：给予你 100% 的控制权，你可以精确地决定要追踪什么、以什么名字追踪。但需要编写更多的模板代码。

*   **自动追踪 (Automatic Tracing via Callbacks)**
    *   **何时使用？** 当你的应用是基于 LangChain 或 LangGraph 构建时。
    *   **如何工作？** 你只需要创建一个 `LangfuseCallbackHandler` 的实例，并将其传入 LangChain 的调用配置中。Handler 会自动“挂钩”到 LangChain 的事件系统上，将所有内部事件（链的开始/结束、工具的调用等）自动转换为 Langfuse 的 Trace 和 Span。
    *   **优缺点**：极其方便，对现有代码的侵入性几乎为零。但其追踪的结构和命名由 LangChain 的内部事件决定，自定义程度较低。

## 2. 场景一：追踪原生 OpenAI 应用

**目标**：追踪一个实现了 Function Calling 的天气查询机器人。

```python
# trace_openai_app.py

import os
from openai import OpenAI
import json
from langfuse import Langfuse

# --- 1. 初始化客户端 ---
client = OpenAI()
langfuse = Langfuse() # 从环境变量自动加载配置

# --- 2. 模拟的工具函数 ---
def get_current_weather(location: str, unit: str = "celsius"):
    """模拟获取天气的工具"""
    # ... (函数实现)
    return {"location": location, "temperature": "25", "unit": unit}

# --- 3. 要追踪的业务逻辑 ---
def run_conversation(user_prompt: str):
    
    # 3.1 使用 langfuse.trace() 包裹整个请求
    trace = langfuse.trace(
        name="openai-native-conversation",
        user_id="user-002",
        session_id="session-abc-123",
        metadata={"use_case": "native_openai_example"}
    )
    
    messages = [{"role": "user", "content": user_prompt}]
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                # ... (工具的 JSON Schema 定义)
            }
        }
    ]

    # 3.2 使用 trace.generation() 手动记录 LLM 调用
    generation = trace.generation(
        name="planner-generation",
        model="gpt-4-turbo",
        input=messages,
        model_parameters={"tools": tools}
    )
    
    response = client.chat.completions.create(
        model="gpt-4-turbo", messages=messages, tools=tools
    )
    response_message = response.choices[0].message
    
    # 记录 LLM 调用的输出和 token 用量
    generation.end(output=response_message, usage=response.usage)
    
    # 3.3 检查是否有工具调用
    if response_message.tool_calls:
        tool_call = response_message.tool_calls[0]
        function_name = tool_call.function.name
        function_args = json.loads(tool_call.function.arguments)

        # 3.4 使用 trace.span() 手动记录工具的执行
        with trace.span(
            name="tool-execution",
            input={"name": function_name, "args": function_args},
            metadata={"tool_type": "api_call"}
        ) as span:
            tool_output = get_current_weather(**function_args)
            span.end(output=tool_output)

        # 3.5 (可选) 再次调用 LLM 进行总结
        # ... (这里会是另一次 trace.generation() 调用)
    
    # 3.6 确保所有数据被发送
    langfuse.flush()
    print("Trace for native OpenAI app sent to Langfuse.")

if __name__ == "__main__":
    run_conversation("What is the weather in Berlin?")

```
**关键点**：
*   整个流程被一个 `trace` 对象所包裹。
*   每次对 OpenAI API 的调用，都对应一次手动的 `trace.generation()`，我们需要手动传入 `input`, `output`, `usage` 等信息。
*   每次对我们自己工具的调用，都对应一次手动的 `trace.span()`。

手动追踪虽然代码量稍多，但给予了我们最大的灵活性，我们可以为每个 `span` 或 `generation` 附加任意的 `metadata`，精确地命名每一个步骤。

## 3. 场景二：追踪标准 LangChain 应用

**目标**：追踪一个使用 `AgentExecutor` 构建的 ReAct 风格 Agent（与 `2.2` 节中的 `multi_tool_agent` 类似）。

```python
# trace_langchain_app.py

import os
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage
from langfuse.callback import LangfuseCallbackHandler

# --- 1. 设置 Langfuse Handler ---
# 这是你唯一需要增加的代码！
langfuse_handler = LangfuseCallbackHandler(
    session_id="langchain-agent-session-001",
    tags=["langchain", "agent-executor"]
)

# --- 2. 你的标准 LangChain Agent 代码 (无需任何改动) ---
tools = [TavilySearchResults(max_results=2)]
llm = ChatOpenAI(model="deepseek-chat")
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}"),
    ("placeholder", "{agent_scratchpad}")
])
agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# --- 3. 在调用时传入 Callback ---
if __name__ == "__main__":
    print("Tracing LangChain AgentExecutor...")
    response = agent_executor.invoke(
        {"input": "What is the capital of France?"},
        # 魔法在这里发生
        config={"callbacks": [langfuse_handler]}
    )
    
    print("\nFinal Answer:", response['output'])
    print("Trace sent to Langfuse. Check http://localhost:3000")
```
**关键点**：
*   我们没有使用任何 `langfuse.trace()` 或 `langfuse.span()`。
*   所有追踪逻辑都被封装在 `LangfuseCallbackHandler` 内部。
*   我们唯一要做的，就是在调用 `agent_executor.invoke()` 时，通过 `config` 字典把 `handler` 传进去。

这就是“自动追踪”的魔力。当你去 Langfuse UI 查看时，你会发现它已经为你生成了一个非常详尽、包含正确层级关系的 Trace，完美地记录了 `AgentExecutor` 的每一步 ReAct 循环。

## 4. 场景三：追踪复杂 LangGraph 应用

**目标**：追踪我们第三周的毕业项目“旅小智”（`trip-genius`）。

你可能会以为，对于 LangGraph 这样复杂的、包含多个 Agent 协作的图，集成过程会很复杂。但事实是...

**它和追踪简单的 LangChain 应用完全一样！**

**代码实战**：

假设你已经有了 `trip-genius` 项目的 `agents/graph.py` 文件，其中编译好的 `app` 已经可用。

```python
# trace_langgraph_app.py

import os
from langfuse.callback import LangfuseCallbackHandler

# 1. 导入你已经编译好的 LangGraph App
#    确保这个文件能被 Python 找到
from agents.graph import app 

# 2. 创建 Langfuse Handler
langfuse_handler = LangfuseCallbackHandler(
    session_id="trip-genius-session-001",
    tags=["langgraph", "multi-agent"]
)

# 3. 在调用时传入 Callback
if __name__ == "__main__":
    print("Tracing LangGraph multi-agent application...")
    
    inputs = {"messages": [("user", "I want to go to Xiamen for 3 days.")]}
    
    # 和 LangChain 的方式完全相同
    for output in app.stream(
        inputs,
        config={"callbacks": [langfuse_handler], "configurable": {"thread_id": "tg-001"}}
    ):
        print(output)
        
    print("\nTrace sent to Langfuse. Check http://localhost:3000")

```

**关键点**：
*   **无需任何额外操作**。因为 LangGraph 本身就是基于 LangChain 的 `Runnable` 协议构建的，所以它完美地兼容 LangChain 的 Callbacks 系统。
*   当你去 Langfuse UI 查看时，你会看到一个更加惊艳的 Trace。LangGraph 的每个**节点**（如 `Supervisor`, `CityExpert`）都会被记录为一个父 `Span`，而节点内部的 LLM 调用和工具调用则会作为子 `Span` 和 `Generation` 嵌套在其中。这让你能以上帝视角，清晰地看到整个 AI 团队是如何一步步协同工作的。

## 5. 追踪元数据（Metadata）：为你的 Trace 添加“标签”和“上下文”

仅仅记录调用链是不够的。为了方便后续的分析和筛选，我们需要为 Trace 附加丰富的“元数据”。

*   **为什么要元数据？**
    *   **筛选**：我想查看所有 `user_id="user-007"` 的调用。
    *   **分组**：我想对比 `version="v1"` 和 `version="v2"` 的 Prompt 在评估集上的平均得分。
    *   **分析**：我想统计所有 `tags=["production"]` 的 Trace 的 P99 延迟。

*   **如何添加？**
    *   **手动追踪**：在调用 `langfuse.trace()`, `.span()`, `.generation()` 时，直接传入 `metadata={...}` 字典。
        ```python
        trace = langfuse.trace(
            name="...",
            user_id="user-007",
            release="v2.1.0", # 版本号
            metadata={"environment": "production", "data_source": "real_time_api"}
        )
        ```
    *   **自动追踪**：在初始化 `LangfuseCallbackHandler` 时，可以传入一些全局的元数据。
        ```python
        handler = LangfuseCallbackHandler(
            user_id="user-007",
            session_id="session-abc",
            tags=["langchain", f"release:{os.getenv('APP_VERSION')}"]
        )
        ```
        这些信息会被自动附加到该 `handler` 追踪的所有 Trace 上。

## 6. 总结：一个 Handler，追踪万物

通过本章的实战，我们得出了一个极其强大的结论：

对于所有基于 LangChain 生态（包括 LangChain `Chain`, `AgentExecutor`, 和 LangGraph `StatefulGraph`）构建的应用，我们只需要一个 `LangfuseCallbackHandler`，就能实现“一行代码，完全追踪”。

这极大地降低了可观测性集成的门槛，让我们可以将更多精力放在应用逻辑本身，而不是繁琐的埋点和日志记录上。现在，我们已经为我们的所有 AI 应用都装上了“黑匣子”，下一步，我们将学习如何利用 Langfuse 记录下来的这些宝贵数据，进行科学、量化的评估。
